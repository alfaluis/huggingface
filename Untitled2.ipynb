{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexa Spanish Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset massive (/root/.cache/huggingface/datasets/qanastek___massive/es-ES/1.0.0/31cdffab94ac97bfe5a394b1e96344c96f0ad847e1d796c7562d8c8b449e22e6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac27246c47434069a9ba44c7d6f3140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'tokens', 'ner_tags', 'worker_id', 'slot_method', 'judgments'],\n",
      "    num_rows: 11514\n",
      "})\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59} 60\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"qanastek/MASSIVE\", \"es-ES\")\n",
    "num_classes = len(set(dataset['train']['intent']))\n",
    "\n",
    "print(dataset['train'])\n",
    "print(set(dataset['train']['intent']), num_classes)\n",
    "\n",
    "model_name = 'qanastek/XLMRoberta-Alexa-Intents-Classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5713, -1.3427,  0.8039, -0.8721,  0.5726, -0.2430, -0.5294, -0.6865,\n",
      "          0.1074,  0.2478,  0.3840,  1.6689, -1.6964,  0.1880, -1.1068,  0.7742,\n",
      "          0.1677, -1.0114, -0.0617,  1.3385, -0.5533,  2.6357, -1.8801, -2.3369,\n",
      "          1.5730,  1.1780,  0.3283, -1.9704, -0.8623,  0.2612, -1.5377,  1.0328,\n",
      "          0.6815, -1.4394,  0.2584, -0.9833,  1.1625, -0.0218, -1.7420, -0.5813,\n",
      "          1.7428,  1.7015, -1.2141, -1.0900,  0.2160, -1.2087,  0.6562, -0.2998,\n",
      "         -1.0372,  0.6882, -0.1992, -0.6526,  0.9215,  0.0484, -1.2738, 13.2861,\n",
      "          2.8905,  0.3491, -0.5015,  1.6039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'qanastek/XLMRoberta-Alexa-Intents-Classification'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "raw_inputs = [\"despiértame a las nueve de la mañana el viernes\"]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spanish dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset massive (/root/.cache/huggingface/datasets/qanastek___massive/es-ES/1.0.0/31cdffab94ac97bfe5a394b1e96344c96f0ad847e1d796c7562d8c8b449e22e6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7bb09dbd7e4904993b3d2e238e9407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'tokens', 'ner_tags', 'worker_id', 'slot_method', 'judgments'],\n",
      "    num_rows: 11514\n",
      "})\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

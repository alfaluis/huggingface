{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higging Pipeline blocks Understanding\n",
    "\n",
    "Models in Hugging (pipeline) are compose of 3 main blocks:\n",
    "- **tokenizer**: Map text to tokens ID\n",
    "- **Model**: Get token ID, map to token vector and transform it to retrieve a high-dimensional vector representing  contextual understanding\n",
    "- **Head**: The final classifier for our task (classification, answer question, masked word, etc)\n",
    "\n",
    "![title](./HuggingBlockStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each step expanded\n",
    "\n",
    "### 1- Tokenizer\n",
    "\n",
    "The first step is use a tokenizer model to transform our plain text into token IDs based in its dictionary\n",
    "\n",
    "Some important details:\n",
    "- **Padding**: add a special character to make all sentence with the same number of words\n",
    "- **Attention Mask**: Tensor with 0's and 1's, where 1 indicate tokens should be attended to, and 0 ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special token padding ID: 0\n",
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "print('special token padding ID:', tokenizer.pad_token_id)\n",
    "\n",
    "raw_inputs = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Tokenization step by step\n",
    "We can do the same step by step if we want get more details about what is happening behind scence.\n",
    "\n",
    "**NOTE:** Be careful because using this manual procedure we don't include special tokens like 101 -> [CLS] and 102 -> [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "i've been waiting for a huggingface course my whole life.\n",
      "tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(1, 14), dtype=int32)\n",
      "Logits: tf.Tensor([[-2.7276194  2.8789368]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# split sentence by model definition (distibert)\n",
    "tokens = tokenizer.tokenize(raw_inputs[0])\n",
    "print(tokens)\n",
    "\n",
    "# convert tokens to ID based in dictionary.\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# come back to string from token ID\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)\n",
    "\n",
    "model_inputs = tf.constant([ids])\n",
    "print(model_inputs)\n",
    "\n",
    "output = model(model_inputs)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Transform network\n",
    "\n",
    "This block of code, get the token ID, use the dictionary to embedding into representitive token vectors and the subsequence layers manipulate it to generete the final high dimensional vector that represent the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ef94af62af44d7b303c47992043c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16, 768)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the class AutoModel we get the hidden responde or the sentence's vector representation. From here, we should be able to use this \"features\" to create our model for the specific task\n",
    "\n",
    "For example, hugging provide plenty of models for kwown task, like:\n",
    "\n",
    "- Model (retrieve the hidden states)\n",
    "- ForCausalLM\n",
    "- ForMaskedLM\n",
    "- ForMultipleChoice\n",
    "- ForQuestionAnswering\n",
    "- ForSequenceClassification\n",
    "- ForTokenClassification\n",
    "\n",
    "Hence, if we are interested in classify the sentence in pos/neg we can use a Sequence Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[-1.5606973,  1.612282 ],\n",
      "       [ 4.1692305, -3.3464472]], dtype=float32)>, hidden_states=None, attentions=None)\n",
      "tf.Tensor(\n",
      "[[-1.5606973  1.612282 ]\n",
      " [ 4.1692305 -3.3464472]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoModelForSequenceClassification include the Model and Head for the specific task. As a result, it return the logic prediction, we just need to convert to probabilities through softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions: tf.Tensor(\n",
      "[[4.0195312e-02 9.5980465e-01]\n",
      " [9.9945587e-01 5.4418476e-04]], shape=(2, 2), dtype=float32)\n",
      "Model labels order: {0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "\n",
    "print('model predictions:', predictions)\n",
    "print('Model labels order:', model.config.id2label)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
